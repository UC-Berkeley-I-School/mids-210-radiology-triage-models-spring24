{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4fc27c-e8b7-4bf5-9c8d-75024f3ef3eb",
   "metadata": {},
   "source": [
    "# Late Fusion Model Embeddings Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff28a5-34a2-43d1-a346-8d287585e8f6",
   "metadata": {},
   "source": [
    "Purpose of this notebook: Generate embeddings for Late Fusion Model\n",
    "\n",
    "Based on paper: Soenksen, L.R., Ma, Y., Zeng, C. et al. Integrated multimodal artificial intelligence framework for healthcare applications. npj Digit. Med. 5, 149 (2022). https://doi.org/10.1038/s41746-022-00689-4\n",
    "\n",
    "Using our best tabular, notes, and imaging models, getting the embeddings and classifications for additional modelling\n",
    "\n",
    "Goal: model using embeddings alone, or embeddings + classification (approach done in Soenksen et al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f24a44-f617-409a-8d5d-6eed11bf8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant code chunks to adapt to our purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "89008c47-674c-41a4-8967-d0b7416a3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports adapted from HAIM API.py\n",
    "# Base\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import datetime as dt\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "# Core AI/ML\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision, torchvision.transforms #causing problems\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "# Scipy\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "\n",
    "# NLP\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "logging.set_verbosity_error()\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "import skimage, skimage.io\n",
    "import torchxrayvision as xrv\n",
    "import timm\n",
    "\n",
    "# Warning handling\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db65bb-c000-4ba0-bdbb-2045734eb63d",
   "metadata": {},
   "source": [
    "# Data Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aa367fb9-3400-4939-b395-92e362083079",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variables_dict = {\n",
    "    'no_finding': 0,\n",
    "    'atelectasis': 1,\n",
    "    'cardiomegaly': 2,\n",
    "    'lung_opacity': 3,\n",
    "    'pleural_effusion': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "42b58121-cb38-40eb-9eb4-31852341d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files... C:/Users/Carolyn/Documents/MIDS/210 Capstone/fusion_data\\test_set__chexpert__4_findings__single_label__unbalanced.json test\n",
      "Loading data files... C:/Users/Carolyn/Documents/MIDS/210 Capstone/fusion_data\\train_set__chexpert__4_findings__single_label__balanced.json train\n",
      "Loading data files... C:/Users/Carolyn/Documents/MIDS/210 Capstone/fusion_data\\validation_set__chexpert__4_findings__single_label__unbalanced.json validate\n",
      "Total Cols\n",
      " Index(['patient_id', 'visit_id', 'study_id', 'temperature', 'heartrate',\n",
      "       'resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity',\n",
      "       'positive_label_total', 'finding_names', 'radiology_note',\n",
      "       'discharge_note', 'chief_complaint',\n",
      "       'major_surgical_or_invasive_procedure', 'history_of_present_illness',\n",
      "       'past_medical_history', 'family_history', 'atelectasis', 'cardiomegaly',\n",
      "       'lung_opacity', 'pleural_effusion', 'dataset_type'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16877 entries, 0 to 16876\n",
      "Data columns (total 40 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   patient_id                                  16877 non-null  int64  \n",
      " 1   study_id                                    16877 non-null  int64  \n",
      " 2   atelectasis                                 16877 non-null  float64\n",
      " 3   cardiomegaly                                16877 non-null  float64\n",
      " 4   edema                                       16877 non-null  float64\n",
      " 5   lung_opacity                                16877 non-null  float64\n",
      " 6   pleural_effusion                            16877 non-null  float64\n",
      " 7   pneumonia                                   16877 non-null  float64\n",
      " 8   prev_data_type                              16877 non-null  object \n",
      " 9   prev_is_sample                              16877 non-null  bool   \n",
      " 10  dicom_id                                    16877 non-null  object \n",
      " 11  PerformedProcedureStepDescription           16817 non-null  object \n",
      " 12  ViewPosition                                16877 non-null  object \n",
      " 13  Rows                                        16877 non-null  int64  \n",
      " 14  Columns                                     16877 non-null  int64  \n",
      " 15  StudyDate                                   16877 non-null  int64  \n",
      " 16  StudyTime                                   16877 non-null  float64\n",
      " 17  ProcedureCodeSequence_CodeMeaning           16877 non-null  object \n",
      " 18  ViewCodeSequence_CodeMeaning                16877 non-null  object \n",
      " 19  PatientOrientationCodeSequence_CodeMeaning  16520 non-null  object \n",
      " 20  jpg_filename                                16877 non-null  object \n",
      " 21  anomaly                                     16877 non-null  int64  \n",
      " 22  no_findings                                 16877 non-null  bool   \n",
      " 23  set                                         16877 non-null  int64  \n",
      " 24  train_6_unb_m                               16877 non-null  int64  \n",
      " 25  test_6_unb_m                                16877 non-null  int64  \n",
      " 26  val_6_unb_m                                 16877 non-null  int64  \n",
      " 27  train_6_bal_m                               16877 non-null  int64  \n",
      " 28  train_6_unb_s                               16877 non-null  int64  \n",
      " 29  test_6_unb_s                                16877 non-null  int64  \n",
      " 30  val_6_unb_s                                 16877 non-null  int64  \n",
      " 31  train_6_bal_s                               16877 non-null  int64  \n",
      " 32  train_4_unb_m                               16877 non-null  int64  \n",
      " 33  test_4_unb_m                                16877 non-null  int64  \n",
      " 34  val_4_unb_m                                 16877 non-null  int64  \n",
      " 35  train_4_bal_m                               16877 non-null  int64  \n",
      " 36  train_4_unb_s                               16877 non-null  int64  \n",
      " 37  test_4_unb_s                                16877 non-null  int64  \n",
      " 38  val_4_unb_s                                 16877 non-null  int64  \n",
      " 39  train_4_bal_s                               16877 non-null  int64  \n",
      "dtypes: bool(2), float64(7), int64(23), object(8)\n",
      "memory usage: 4.9+ MB\n",
      "\n",
      "\n",
      "Create combined train, validation, and test datasets from note/tabular and imaging data of sizes: \n",
      "(2086, 27)\n",
      "(1924, 27)\n",
      "(1920, 27)\n",
      "Null values for image IDs in combined_train_df: 0\n",
      "Null values for image IDs in combined_train_df: 0\n",
      "Null values for image IDs in combined_train_df: 0\n",
      "\n",
      "\n",
      "Combined dataset has columns: Index(['index', 'patient_id', 'visit_id', 'study_id', 'temperature',\n",
      "       'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity',\n",
      "       'positive_label_total', 'finding_names', 'radiology_note',\n",
      "       'discharge_note', 'chief_complaint',\n",
      "       'major_surgical_or_invasive_procedure', 'history_of_present_illness',\n",
      "       'past_medical_history', 'family_history', 'atelectasis', 'cardiomegaly',\n",
      "       'lung_opacity', 'pleural_effusion', 'dataset_type', 'dicom_id'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Combined dataset has the following findings (true labels): ['atelectasis' 'cardiomegaly' 'lung_opacity' 'pleural_effusion'\n",
      " 'no_finding']\n",
      "\n",
      "\n",
      "Split combined data into images, notes, and tabular datasets\n",
      "Imaging data has columns Index(['patient_id', 'dicom_id', 'finding_names'], dtype='object')\n",
      "Notes data has columns Index(['patient_id', 'chief_complaint', 'history_of_present_illness',\n",
      "       'past_medical_history', 'family_history', 'finding_names'],\n",
      "      dtype='object')\n",
      "Tabular data has columns Index(['patient_id', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp',\n",
      "       'dbp', 'pain', 'acuity', 'finding_names'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Processed all notes data to have [UNK] in place of ____\n",
      "\n",
      "\n",
      "Created file img_train_df with size: (2086, 3)\n",
      "Created file img_val_df with size: (1924, 3)\n",
      "Created file img_test_df with size: (1920, 3)\n",
      "Created file notes_train_df with size: (2086, 6)\n",
      "Created file notes_val_df with size: (1924, 6)\n",
      "Created file notes_test_df with size: (1920, 6)\n",
      "Created file tabular_train_df with size: (2086, 10)\n",
      "Created file tabular_val_df with size: (1924, 10)\n",
      "Created file tabular_test_df with size: (1920, 10)\n"
     ]
    }
   ],
   "source": [
    "#run processing notebook to get data files\n",
    "%run processing_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eca75f-5181-43f4-84f3-c1e8f0db9bc4",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596452e-7801-4195-919a-9e1d7e32fa69",
   "metadata": {},
   "source": [
    "## Notes Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263ac2a-5464-4449-8b83-17604b1823ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original biobert models\n",
    "biobert_tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "biobert_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4847147a-3d28-45e5-9a4d-1c90501456e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Bio_Discharge_Summary_BERT model\n",
    "MODEL_CHECKPOINT = 'emilyalsentzer/Bio_ClinicalBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f5c6047a-55ee-47b8-a341-cd25c62acc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parameters\n",
    "NUM_CLASSES = 5\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fb93943a-9d69-4e8d-8572-f6630f9f8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, checkpoint, num_classes, hidden_size=201, dropout_prob=0.3, freeze_bert=True):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(checkpoint)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.num_classes = num_classes\n",
    "        self.freeze_bert = freeze_bert\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = not self.freeze_bert\n",
    "\n",
    "        self.pooler_layer = nn.Linear(self.model.config.hidden_size, hidden_size) # maps the output of the BERT model's hidden state to the hidden_size\n",
    "        self.relu = nn.ReLU() # introduces non-linearity to the model\n",
    "        self.dropout = nn.Dropout(dropout_prob) # applied for regularization\n",
    "        self.classification_layer = nn.Linear(hidden_size, num_classes) # projects the hidden_size down to the number of target classes\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids = None, attention_mask = None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask)\n",
    "\n",
    "        pooler_output = outputs.pooler_output\n",
    "        hidden = self.pooler_layer(pooler_output)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        classification = self.classification_layer(hidden) # logits for each class\n",
    "\n",
    "        return classification, hidden ###STEVEN - ONLY CHANGE I MADE TO THE MODEL\n",
    "\n",
    "    def unfreeze_bert_layers(self, n_layers):\n",
    "        \"\"\"Unfreezes the top n layers of the BERT model.\"\"\"\n",
    "        layers_to_unfreeze = list(self.model.encoder.layer[-n_layers:])\n",
    "        for layer in layers_to_unfreeze:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "da40ac41-2025-477a-9657-a4d54db5976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, checkpoint_path):\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "eae8ed5d-c1e5-4842-845b-1cc0655c1290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_FOLDER = \"./checkpoints\"\n",
    "MODEL_NAME_FOLDER = \"./model_findings\"\n",
    "\n",
    "esteban_model = MulticlassClassification(\n",
    "    checkpoint=MODEL_CHECKPOINT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    freeze_bert=False,\n",
    "    )\n",
    "\n",
    "CHECKPOINT_FILE = r\"C:\\Users\\Carolyn\\Documents\\MIDS\\210 Capstone\\mids-210-radiology-triage-models-spring24\\pretrained_weights\\bio_clinical_bert\\bio_clinical_bert__balanced__unfrozen_layers__best.pt\"\n",
    "\n",
    "esteban_model.load_state_dict(torch.load(CHECKPOINT_FILE, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cb8ca80a-a29a-4c74-b4af-b501cf1bd83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassClassification(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooler_layer): Linear(in_features=768, out_features=201, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classification_layer): Linear(in_features=201, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esteban_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f749e-5437-4a3f-bcac-6dd2cc3c6eb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "23f07a6c-0b30-4542-8e22-0668967471da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biobert_classifications(text, model = \"MMMM\"):\n",
    "    # Inputs:\n",
    "    #   text -> Input text (str)\n",
    "    #\n",
    "    # Outputs:\n",
    "    #   classifications -> Final Biobert classifications = (1,num_classifcations)\n",
    "    #   hidden_embeddings -> Last hidden layer in Biobert model with vector dimensionality = (1,hidder_layer_size)\n",
    "  \n",
    "    # %% EXAMPLE OF USE\n",
    "    # classifications, hidden_embeddings = get_biobert_embeddings(text)\n",
    "    if model == \"HAIM\": #to test the HAIM model\n",
    "        model = biobert_model\n",
    "    \n",
    "        tokens_pt = biobert_tokenizer(text, return_tensors=\"pt\")\n",
    "        outputs = model(**tokens_pt)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooler_output = outputs.pooler_output\n",
    "        classifications = pooler_output.detach().numpy()\n",
    "        \n",
    "    else:\n",
    "        model = esteban_model\n",
    "\n",
    "        tokens_pt = biobert_tokenizer(text, return_tensors='pt',  add_special_tokens=True, padding='max_length', max_length=MAX_SEQUENCE_LENGTH, truncation=True) # (input_ids, attention_mask, token_type_ids)\n",
    "        outputs, hidden_outputs = model(**tokens_pt)\n",
    "        classifications = outputs.detach().numpy()\n",
    "\n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a547ea6f-e431-4ee8-b208-34facb7696e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biobert_embeddings(text, model = \"MMMM\"):\n",
    "    # Inputs:\n",
    "    #   text -> Input text (str)\n",
    "    #\n",
    "    # Outputs:\n",
    "    #   classifications -> Final Biobert classifications = (1,num_classifcations)\n",
    "    #   hidden_embeddings -> Last hidden layer in Biobert model with vector dimensionality = (1,hidder_layer_size)\n",
    "  \n",
    "    # %% EXAMPLE OF USE\n",
    "    # classifications, hidden_embeddings = get_biobert_embeddings(text)\n",
    "    if model == \"HAIM\": #to test the HAIM model\n",
    "        model = biobert_model\n",
    "    \n",
    "        tokens_pt = biobert_tokenizer(text, return_tensors=\"pt\")\n",
    "        outputs = model(**tokens_pt)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooler_output = outputs.pooler_output\n",
    "        hidden_embeddings = last_hidden_state.detach().numpy()\n",
    "        \n",
    "    else:\n",
    "        model = esteban_model\n",
    "\n",
    "        tokens_pt = biobert_tokenizer(text, return_tensors='pt',  add_special_tokens=True, padding='max_length', max_length=MAX_SEQUENCE_LENGTH, truncation=True) # (input_ids, attention_mask, token_type_ids)\n",
    "        outputs, hidden_outputs = model(**tokens_pt)\n",
    "        hidden_embeddings = hidden_outputs.detach().numpy()\n",
    "\n",
    "    return hidden_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ff597939-799b-4acc-a439-c5603eb7ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on EC2 instance\n",
    "#notes_train_df['notes_classifications'] = notes_train_df.history_of_present_illness.apply(get_biobert_classifications)\n",
    "#notes_train_df['notes_embeddings'] = notes_train_df.history_of_present_illness.apply(get_biobert_embeddings)\n",
    "#notes_val_df['notes_classifications'] = notes_val_df.history_of_present_illness.apply(get_biobert_classifications)\n",
    "#notes_val_df['notes_embeddings'] = notes_val_df.history_of_present_illness.apply(get_biobert_embeddings)\n",
    "#notes_test_df['notes_classifications'] = notes_test_df.history_of_present_illness.apply(get_biobert_classifications)\n",
    "#notes_test_df['notes_embeddings'] = notes_test_df.history_of_present_illness.apply(get_biobert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a88774bc-f68c-42b0-996d-8696a1a49c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2086, 6)\n",
      "(1924, 6)\n",
      "(1920, 6)\n"
     ]
    }
   ],
   "source": [
    "print(notes_train_df.shape)\n",
    "print(notes_val_df.shape)\n",
    "print(notes_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a34d7e-e465-4a87-be50-a2ce226108b3",
   "metadata": {},
   "source": [
    "# Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f8516ac2-5f3b-46fb-a715-bf9923523673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Cinthya's model\n",
    "cinthya_model = torch.load(r\"C:\\Users\\Carolyn\\Documents\\MIDS\\210 Capstone\\mids-210-radiology-triage-models-spring24\\pretrained_weights\\EfficientNet_B3.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "fb65dc4c-b884-4178-b8ca-5a9713103c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNormAct2d(\n",
       "    40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): SiLU(inplace=True)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pw): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNormAct2d(\n",
       "    1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): SiLU(inplace=True)\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (classifier): Linear(in_features=1536, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cinthya_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "b6e28cd8-8c23-45fa-8369-d154a4f1efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images embeddings\n",
    "def get_single_chest_xray_embeddings(img, model=\"HAIM\"):\n",
    "    # Inputs:\n",
    "    #   img -> Image array\n",
    "    #\n",
    "    # Outputs:\n",
    "    #   densefeature_embeddings ->  CXR dense feature embeddings for image\n",
    "    #   prediction_embeddings ->  CXR embeddings of predictions for image\n",
    "    \n",
    "    # %% EXAMPLE OF USE\n",
    "    # densefeature_embeddings, prediction_embeddings = get_single_chest_xray_embeddings(img)\n",
    "    \n",
    "    # Select if you want to use CUDA support for GPU (optional as it is usually pretty fast even in CPUT)\n",
    "    cuda = False\n",
    "    \n",
    "    # Select model with a String that determines the model to use for Chest Xrays according to https://github.com/mlmed/torchxrayvision\n",
    "    if model == \"MMMM\":\n",
    "        model_state_dict = cinthya_model\n",
    "    else:\n",
    "        model_weights_name = \"densenet121-res224-chex\" # CheXpert (Stanford)\n",
    "    #model_weights_name = \"resnet50-res512-all\" # Resnet only for 512x512 inputs\n",
    "    # NOTE: The all model has every output trained. However, for the other weights some targets are not trained and will predict randomly becuase they do not exist in the training dataset.\n",
    "    \n",
    "    # Extract chest x-ray image embeddings and preddictions\n",
    "    densefeature_embeddings = []\n",
    "    prediction_embeddings = []\n",
    "    \n",
    "    #img = cv2.imread(local_base_dir + r'/' + img)\n",
    "    img = xrv.datasets.normalize(img, 255)\n",
    "\n",
    "    # For each image check if they are 2D arrays\n",
    "    if len(img.shape) > 2:\n",
    "        img = img[:, :, 0]\n",
    "    if len(img.shape) < 2:\n",
    "        print(\"Error: Dimension lower than 2 for image!\")\n",
    "    \n",
    "    # Add color channel for prediction\n",
    "    #Resize using OpenCV\n",
    "    img = cv2.resize(img, (224, 224), interpolation = cv2.INTER_AREA)   \n",
    "    img = img[None, :, :]\n",
    "\n",
    "    #Or resize using core resizer (thows error sometime)\n",
    "    #transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),xrv.datasets.XRayResizer(224)])\n",
    "    #img = transform(img)\n",
    "    if model == \"MMMM\":\n",
    "        model = Efficient\n",
    "    else:\n",
    "        model = xrv.models.DenseNet(weights = model_weights_name)    # model = xrv.models.ResNet(weights=\"resnet50-res512-all\") # ResNet is also available\n",
    "\n",
    "    output = {}\n",
    "    with torch.no_grad():\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        if cuda:\n",
    "            img = img.cuda()\n",
    "            model = model.cuda()\n",
    "          \n",
    "        # Extract dense features\n",
    "        feats = model.features(img)\n",
    "        feats = F.relu(feats, inplace=True)\n",
    "        feats = F.adaptive_avg_pool2d(feats, (1, 1))\n",
    "        densefeatures = feats.cpu().detach().numpy().reshape(-1)\n",
    "        densefeature_embeddings = densefeatures\n",
    "\n",
    "        # Extract predicted probabilities of considered 18 classes:\n",
    "        # Get by calling \"xrv.datasets.default_pathologies\" or \"dict(zip(xrv.datasets.default_pathologies,preds[0].detach().numpy()))\"\n",
    "        # ['Atelectasis','Consolidation','Infiltration','Pneumothorax','Edema','Emphysema',Fibrosis',\n",
    "        #  'Effusion','Pneumonia','Pleural_Thickening','Cardiomegaly','Nodule',Mass','Hernia',\n",
    "        #  'Lung Lesion','Fracture','Lung Opacity','Enlarged Cardiomediastinum']\n",
    "        preds = model(img).cpu()\n",
    "        predictions = preds[0].detach().numpy()\n",
    "        prediction_embeddings = predictions  \n",
    "\n",
    "    # Return embeddings\n",
    "    return densefeature_embeddings, prediction_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5166ed5-e1ad-46b4-a405-b893c331ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to for each individual model - save the model to run on images\n",
    "#or save the final output layers (better, more consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e5e323b5-5ca7-4722-9a57-58f1f44f486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_base_dir = r\"C:\\Users\\Carolyn\\Documents\\MIDS\\210 Capstone\\fusion_data\\train_4_bal_s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5454c277-a0d3-4fcb-92af-d4eb48fcda99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7d6ea06e-554c2ccb-4d9ecefe-eb5ca0e0-7049fa19.jpg'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train_df.dicom_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5e00084b-305e-498e-8392-764e27d2901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load a sample image\n",
    "img = cv2.imread(local_base_dir + r'/' + img_train_df.dicom_id[0])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e2b5e433-71fb-42d7-bc38-7f3837818ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EfficientNet' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[272], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m densefeautre_embeddings, prediction_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_single_chest_xray_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMMMM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[271], line 59\u001b[0m, in \u001b[0;36mget_single_chest_xray_embeddings\u001b[1;34m(img, model)\u001b[0m\n\u001b[0;32m     56\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Extract dense features\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m(img)\n\u001b[0;32m     60\u001b[0m feats \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(feats, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m feats \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(feats, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "densefeautre_embeddings, prediction_embeddings = get_single_chest_xray_embeddings(img, model = \"MMMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f5643-8079-4367-a5e3-6af3794c5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "densefeautre_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16e4c71c-3bcc-49af-b502-e4121bdb8faf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EfficientNet' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\serialization.py:541\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m densefeature_embeddings_MFA, prediction_embeddings_MFA \u001b[38;5;241m=\u001b[39m \u001b[43mget_single_chest_xray_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMMMM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 57\u001b[0m, in \u001b[0;36mget_single_chest_xray_embeddings\u001b[1;34m(img, model)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMMMM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     56\u001b[0m     model \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefficientnet_b3\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     58\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\serialization.py:450\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[1;32m--> 435\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\serialization.py:544\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 544\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\serialization.py:537\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[1;34m(patterns, e)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    534\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "densefeature_embeddings_MFA, prediction_embeddings_MFA = get_single_chest_xray_embeddings(img, model = \"MMMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065c54e-f648-44ed-b13e-4dd039f17070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72777b7a-6ea7-44de-af58-850d83dacd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>finding_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11388716</td>\n",
       "      <td>7d6ea06e-554c2ccb-4d9ecefe-eb5ca0e0-7049fa19.jpg</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11539363</td>\n",
       "      <td>b4b4f64f-ba5163c8-1b7ce58e-a0030af6-4d09dec1.jpg</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10833304</td>\n",
       "      <td>3b09e882-c5f0d93b-106c7c94-29b839e8-00e7a950.jpg</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19849119</td>\n",
       "      <td>3100385d-f9e0610a-e32f54df-3c17088b-4da3d491.jpg</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11749991</td>\n",
       "      <td>c83dc36c-6c58d087-0ef18130-dd3e8cbf-faa5e609.jpg</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id                                          dicom_id finding_names\n",
       "0    11388716  7d6ea06e-554c2ccb-4d9ecefe-eb5ca0e0-7049fa19.jpg   atelectasis\n",
       "1    11539363  b4b4f64f-ba5163c8-1b7ce58e-a0030af6-4d09dec1.jpg   atelectasis\n",
       "2    10833304  3b09e882-c5f0d93b-106c7c94-29b839e8-00e7a950.jpg   atelectasis\n",
       "3    19849119  3100385d-f9e0610a-e32f54df-3c17088b-4da3d491.jpg   atelectasis\n",
       "4    11749991  c83dc36c-6c58d087-0ef18130-dd3e8cbf-faa5e609.jpg   atelectasis"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train_df_sample = img_train_df.loc[0:5,]\n",
    "img_train_df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afc18e83-7840-4a53-be87-fa4dabd94fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b6b7e8c-f8f5-4dc4-8b2e-9a849f8b4159",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img_train_df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensefeature_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m], img_train_df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m img_train_df_sample\u001b[38;5;241m.\u001b[39mdicom_id\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_single_chest_xray_embeddings(x))\n\u001b[0;32m      2\u001b[0m img_train_df_sample\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for i in len(img_train_df_sample)-1:\n",
    "    \n",
    "    img_train_df['densefeature_embeddings'], img_train_df_sample['prediction_embeddings'] = get_single_chest_xray_embeddings(patient)\n",
    "img_train_df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf327e-3ed7-47a6-a19c-3c8d8f666ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chest_xray_embeddings(dt_patient, model=\"HAIM\", verbose=0):\n",
    "    # Inputs:\n",
    "    #   dt_patient -> Timebound ICU patient stay structure filtered by max_time_stamp or min_time_stamp if any\n",
    "    #   verbose -> Level of printed output of function\n",
    "    #\n",
    "    # Outputs:\n",
    "    #   aggregated_densefeature_embeddings -> CXR aggregated dense feature embeddings for all images in timebound patient\n",
    "    #   densefeature_embeddings ->  List of CXR dense feature embeddings for all images\n",
    "    #   aggregated_prediction_embeddings -> CXR aggregated embeddings of predictions for all images in timebound patient\n",
    "    #   prediction_embeddings ->  List of CXR embeddings of predictions for all images\n",
    "    #   imgs_weights ->  Array of weights for embedding aggregation\n",
    "\n",
    "\n",
    "    # %% EXAMPLE OF USE\n",
    "    # aggregated_densefeature_embeddings, densefeature_embeddings, aggregated_prediction_embeddings, prediction_embeddings, imgs_weights = get_chest_xray_embeddings(dt_patient, verbose=2)\n",
    "\n",
    "    # Clean out process bar before starting\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Select if you want to use CUDA support for GPU (optional as it is usually pretty fast even in CPUT)\n",
    "    cuda = False\n",
    "\n",
    "    # Select model with a String that determines the model to use for Chest Xrays according to https://github.com/mlmed/torchxrayvision\n",
    "    #   model_weights_name = \"densenet121-res224-all\" # Every output trained for all models\n",
    "    #   model_weights_name = \"densenet121-res224-rsna\" # RSNA Pneumonia Challenge\n",
    "    #model_weights_name = \"densenet121-res224-nih\" # NIH chest X-ray8\n",
    "    #model_weights_name = \"densenet121-res224-pc\") # PadChest (University of Alicante)\n",
    "    if model == \"HAIM\":\n",
    "        model_weights_name = \"densenet121-res224-chex\" # CheXpert (Stanford)\n",
    "    elif model == \"EffNet\":\n",
    "        pass\n",
    "    else:\n",
    "        model_weights_name = \"densenet121-res224-chex\" # replace with our model when we can\n",
    "        \n",
    "    #   model_weights_name = \"densenet121-res224-mimic_nb\" # MIMIC-CXR (MIT)\n",
    "    #model_weights_name = \"densenet121-res224-mimic_ch\") # MIMIC-CXR (MIT)\n",
    "    #model_weights_name = \"resnet50-res512-all\" # Resnet only for 512x512 inputs\n",
    "    # NOTE: The all model has every output trained. However, for the other weights some targets are not trained and will predict randomly becuase they do not exist in the training dataset.\n",
    "\n",
    "\n",
    "    # Extract chest x-ray images from timebound patient and iterate through them\n",
    "    imgs = dt_patient.dicom_id\n",
    "    densefeature_embeddings = []\n",
    "    prediction_embeddings = []\n",
    "\n",
    "    # Iterate\n",
    "    nImgs = len(imgs)\n",
    "    with tqdm(total = nImgs) as pbar:\n",
    "        for idx, img in enumerate(imgs):\n",
    "            img = skimage.io.imread(img_path) # If importing from path use this\n",
    "            img = xrv.datasets.normalize(img, 255)\n",
    "          \n",
    "            # For each image check if they are 2D arrays\n",
    "            if len(img.shape) > 2:\n",
    "                img = img[:, :, 0]\n",
    "            if len(img.shape) < 2:\n",
    "                print(\"Error: Dimension lower than 2 for image!\")\n",
    "\n",
    "            # Add color channel for prediction\n",
    "            #Resize using OpenCV\n",
    "            img = cv2.resize(img, (224, 224), interpolation = cv2.INTER_AREA)   \n",
    "            img = img[None, :, :]\n",
    "            \n",
    "            #Or resize using core resizer (thows error sometime)\n",
    "            #transform = transforms.Compose([xrv.datasets.XRayCenterCrop(),xrv.datasets.XRayResizer(224)])\n",
    "            #img = transform(img)\n",
    "            if model == \"HAIM\":\n",
    "                model = xrv.models.DenseNet(weights = model_weights_name)\n",
    "            elif model == \"MMMM\":\n",
    "                model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=5)\n",
    "            else:\n",
    "                model = xrv.models.DenseNet(weights = model_weights_name)\n",
    "            # model = xrv.models.ResNet(weights=\"resnet50-res512-all\") # ResNet is also available\n",
    "            \n",
    "            output = {}\n",
    "            with torch.no_grad():\n",
    "                img = torch.from_numpy(img).unsqueeze(0)\n",
    "                if cuda:\n",
    "                    img = img.cuda()\n",
    "                    model = model.cuda()\n",
    "              \n",
    "                # Extract dense features\n",
    "                feats = model.features(img)\n",
    "                feats = F.relu(feats, inplace=True)\n",
    "                feats = F.adaptive_avg_pool2d(feats, (1, 1))\n",
    "                densefeatures = feats.cpu().detach().numpy().reshape(-1)\n",
    "                densefeature_embeddings.append(densefeatures) # append to list of dense features for all images\n",
    "                \n",
    "                # Extract predicted probabilities of considered 18 classes:\n",
    "                # Get by calling \"xrv.datasets.default_pathologies\" or \"dict(zip(xrv.datasets.default_pathologies,preds[0].detach().numpy()))\"\n",
    "                # ['Atelectasis','Consolidation','Infiltration','Pneumothorax','Edema','Emphysema',Fibrosis',\n",
    "                #  'Effusion','Pneumonia','Pleural_Thickening','Cardiomegaly','Nodule',Mass','Hernia',\n",
    "                #  'Lung Lesion','Fracture','Lung Opacity','Enlarged Cardiomediastinum']\n",
    "                preds = model(img).cpu()\n",
    "                predictions = preds[0].detach().numpy()\n",
    "                prediction_embeddings.append(predictions) # append to list of predictions for all images\n",
    "            \n",
    "                if verbose >=1:\n",
    "                    # Update process bar\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        \n",
    "    # Get image weights by hours passed from current time to image\n",
    "    orig_imgs_weights = np.asarray(dt_patient.cxr.deltacharttime.values)\n",
    "    adj_imgs_weights = orig_imgs_weights - orig_imgs_weights.min()\n",
    "    imgs_weights = (adj_imgs_weights) / (adj_imgs_weights).max()\n",
    "  \n",
    "    # Aggregate with weighted average of ebedding vector across temporal dimension\n",
    "    try:\n",
    "        aggregated_densefeature_embeddings = np.average(densefeature_embeddings, axis=0, weights=imgs_weights)\n",
    "        if np.isnan(np.sum(aggregated_densefeature_embeddings)):\n",
    "            aggregated_densefeature_embeddings = np.zeros_like(densefeature_embeddings[0])\n",
    "    except:\n",
    "        aggregated_densefeature_embeddings = np.zeros_like(densefeature_embeddings[0])\n",
    "      \n",
    "    try:\n",
    "        aggregated_prediction_embeddings = np.average(prediction_embeddings, axis=0, weights=imgs_weights)\n",
    "        if np.isnan(np.sum(aggregated_prediction_embeddings)):\n",
    "            aggregated_prediction_embeddings = np.zeros_like(prediction_embeddings[0])\n",
    "    except:\n",
    "        aggregated_prediction_embeddings = np.zeros_like(prediction_embeddings[0])\n",
    "      \n",
    "      \n",
    "    if verbose >=2:\n",
    "        x = orig_imgs_weights\n",
    "        y = prediction_embeddings\n",
    "        plt.xlabel(\"Time [hrs]\")\n",
    "        plt.ylabel(\"Disease probability [0-1]\")\n",
    "        plt.title(\"A test graph\")\n",
    "        for i in range(len(y[0])):\n",
    "            plt.plot(x,[pt[i] for pt in y],'o', label = xrv.datasets.default_pathologies[i])\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "        plt.show()\n",
    "\n",
    "    # Return embeddings\n",
    "    return aggregated_densefeature_embeddings, densefeature_embeddings, aggregated_prediction_embeddings, prediction_embeddings, imgs_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65096003-a045-41c3-b671-31a9e4d0c512",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generate Tabular Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8eb0064-eea3-4866-9c7a-1d18a4cd52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabular embeddings\n",
    "#need to build our own models for these\n",
    "#for now, append tabular data similarly to demographic data (as raw values)\n",
    "\n",
    "def get_tabular_embeddings(dt_patient, verbose=0):\n",
    "    # Inputs:\n",
    "    #   dt_patient -> Timebound mimic patient structure\n",
    "    #   verbose -> Flag to print found keyword outputs (0,1)\n",
    "    #\n",
    "    # Outputs:\n",
    "    #   base_embeddings -> Core base embeddings for the selected patient\n",
    "\n",
    "    # %% EXAMPLE OF USE\n",
    "    # base_embeddings = get_demographic_embeddings(dt_patient, verbose=1)\n",
    "\n",
    "    # Retrieve dt_patient and get embeddings \n",
    "    demo_embeddings =  dt_patient[['temperature', 'heartrate','resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity']].values[0]\n",
    "    if verbose >= 1:\n",
    "        print(demo_embeddings)\n",
    "    return demo_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69274413-96be-466f-b0db-c411ab829bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>finding_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11388716</td>\n",
       "      <td>98.8</td>\n",
       "      <td>106.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11539363</td>\n",
       "      <td>99.1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10833304</td>\n",
       "      <td>97.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19849119</td>\n",
       "      <td>98.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11749991</td>\n",
       "      <td>100.6</td>\n",
       "      <td>110.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>atelectasis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  temperature  heartrate  resprate  o2sat    sbp    dbp  pain  \\\n",
       "0    11388716         98.8      106.0      22.0   96.0   93.0   67.0     0   \n",
       "1    11539363         99.1       80.0      16.0   97.0  162.0   67.0     0   \n",
       "2    10833304         97.0       98.0      14.0  100.0  159.0   88.0     2   \n",
       "3    19849119         98.6       92.0      20.0   98.0  127.0   70.0     0   \n",
       "4    11749991        100.6      110.0      16.0   97.0  166.0  100.0     8   \n",
       "\n",
       "   acuity finding_names  \n",
       "0     2.0   atelectasis  \n",
       "1     3.0   atelectasis  \n",
       "2     2.0   atelectasis  \n",
       "3     2.0   atelectasis  \n",
       "4     2.0   atelectasis  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47bb5f3-e983-4a9e-9508-c990acc13db9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Concatenate Embeddings together and save output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbaf616-68e1-4571-925d-6e7f09117570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAIM's concatentation\n",
    "def process_cxr_embeddings_patient_id(patient_id, dt_patient, df_init):\n",
    "    \n",
    "    # TABULAR EMBEDDINGS EXTRACTION\n",
    "    tabular_embeddings = get_tabular_embeddings(dt_patient, verbose=0)\n",
    "    gc.collect() #Clear memory\n",
    "    \n",
    "    # NOTES EMBEDDINGS\n",
    "    aggregated_notes_embeddings, aggregated_notes_hidden_embeddings = get_biobert_embeddings(patient, note_type = 'radnotes')\n",
    "    gc.collect() #Clear memory\n",
    "\n",
    "    # CHEST XRAY VISION EMBEDDINGS EXTRACTION\n",
    "    #aggregated_densefeature_embeddings, _, aggregated_prediction_embeddings, _, _ = get_chest_xray_embeddings(dt_patient, verbose=0)\n",
    "    #gc.collect() #Clear memory\n",
    "    \n",
    "    # CHEST XRAY VISION SINGLE-IMAGE EMBEDDINGS EXTRACTION\n",
    "    print('getting xray')\n",
    "    img = df_imcxr[idx]\n",
    "    densefeature_embeddings, prediction_embeddings = get_single_chest_xray_embeddings(img)\n",
    "    gc.collect() #Clear memory\n",
    "\n",
    "    # Create Dataframes filteed by ordered sample number for Fusion\n",
    "    df_patient_ids_fusion = pd.DataFrame([patient_id],columns=['patient_id'])\n",
    "    df_tabular_embeddings_fusion = pd.DataFrame(tabular_embeddings.reshape(1,-1), columns=['de_'+str(i) for i in range(demo_embeddings.shape[0])])\n",
    "    \n",
    "    df_vision_dense_embeddings_fusion = pd.DataFrame(densefeature_embeddings.reshape(1,-1), columns=['vd_'+str(i) for i in range(densefeature_embeddings.shape[0])])\n",
    "    df_vision_predictions_embeddings_fusion = pd.DataFrame(prediction_embeddings.reshape(1,-1), columns=['vp_'+str(i) for i in range(prediction_embeddings.shape[0])])\n",
    "    df_notes_embeddings_fusion = pd.DataFrame(aggregated_notes_embeddings.reshape(1,-1), columns=['n_rad_'+str(i) for i in range(aggregated_rad_embeddings.shape[0])])\n",
    "    df_notes_hidden_embeddings_fusion = pd.DataFrame(aggregated_notes_hidden_embeddings.reshape(1,-1), columns=['n_rad_'+str(i) for i in range(aggregated_rad_embeddings.shape[0])])\n",
    "\n",
    "    # Vision targets\n",
    "    cxr_target_columns = ['split','Atelectasis','Cardiomegaly','Consolidation','Edema','Enlarged Cardiomediastinum','Fracture','Lung Lesion','Lung Opacity','No Finding','Pleural Effusion','Pleural Other','Pneumonia','Pneumothorax','Support Devices', 'PerformedProcedureStepDescription','ViewPosition']\n",
    "    df_vision_targets_fusion = df_stay_cxr.loc[idx:idx][cxr_target_columns].reset_index(drop=True)\n",
    "\n",
    "    # Embeddings FUSION\n",
    "    df_fusion = df_patient_ids_fusion\n",
    "    df_fusion = pd.concat([df_fusion, df_init], axis=1)\n",
    "    df_fusion = pd.concat([df_fusion, df_tabular_embeddings_fusion], axis=1)\n",
    "    df_fusion = pd.concat([df_fusion, df_vision_dense_embeddings_fusion], axis=1)\n",
    "    df_fusion = pd.concat([df_fusion, df_vision_predictions_embeddings_fusion], axis=1)\n",
    "    df_fusion = pd.concat([df_fusion, df_notes_embeddings_fusion], axis=1)\n",
    "    \n",
    "    #Add targets\n",
    "    df_fusion = pd.concat([df_fusion, df_vision_targets_fusion], axis=1)\n",
    "    gc.collect() #Clear memory\n",
    "    \n",
    "    return df_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f6f4a999-e849-4ce1-adcd-f8a452f0eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tabular \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60c2cd6a-700d-4d8c-8f51-0ea5a69bb4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97.6 117.   18.   95.  128.   74.   10.    3. ]\n"
     ]
    }
   ],
   "source": [
    "test_tabular_embedding = get_tabular_embeddings(train_df, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021ef86-8729-4614-b688-4504800cbe38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
